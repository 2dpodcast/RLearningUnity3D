{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 10000000 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"tictac\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 64 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-2 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.3 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 4096 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 27 # Number of units in hidden layer.\n",
    "batch_size = 32 # How many experiences per gradient descent update step.\n",
    "normalize = True\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tdefence_penalty -> -0.5\n",
      "\t\tdefence_reward -> 0.5\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 10\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 9\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: 0, 1, 2, 3, 4, 5, 6, 7, 8\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-3450000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-3450000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 3460000. Mean Reward: 2.96597353497. Std of Reward: 0.297119198008.\n",
      "Step: 3470000. Mean Reward: 2.97987689394. Std of Reward: 0.228756279919.\n",
      "Step: 3480000. Mean Reward: 2.99243140965. Std of Reward: 0.148914239055.\n",
      "Step: 3490000. Mean Reward: 2.9852661597. Std of Reward: 0.199563748249.\n",
      "Step: 3500000. Mean Reward: 2.97768281102. Std of Reward: 0.254075136121.\n",
      "Saved Model\n",
      "Step: 3510000. Mean Reward: 2.98529411765. Std of Reward: 0.202620542574.\n",
      "Step: 3520000. Mean Reward: 2.97772511848. Std of Reward: 0.222760663192.\n",
      "Step: 3530000. Mean Reward: 2.97865275142. Std of Reward: 0.23887919606.\n",
      "Step: 3540000. Mean Reward: 2.99382716049. Std of Reward: 0.122144017494.\n",
      "Step: 3550000. Mean Reward: 2.98224431818. Std of Reward: 0.226613701823.\n",
      "Saved Model\n",
      "Step: 3560000. Mean Reward: 2.99028436019. Std of Reward: 0.165689467704.\n",
      "Step: 3570000. Mean Reward: 2.99097815764. Std of Reward: 0.129060056169.\n",
      "Step: 3580000. Mean Reward: 2.99337121212. Std of Reward: 0.124349056929.\n",
      "Step: 3590000. Mean Reward: 2.97912713472. Std of Reward: 0.233397066537.\n",
      "Step: 3600000. Mean Reward: 2.99762808349. Std of Reward: 0.0769685993339.\n",
      "Saved Model\n",
      "Step: 3610000. Mean Reward: 2.99453422053. Std of Reward: 0.114454698927.\n",
      "Step: 3620000. Mean Reward: 2.99359582543. Std of Reward: 0.126135344246.\n",
      "Step: 3630000. Mean Reward: 2.97910731244. Std of Reward: 0.246129315515.\n",
      "Step: 3640000. Mean Reward: 2.99383301708. Std of Reward: 0.107630661122.\n",
      "Step: 3650000. Mean Reward: 2.97849716446. Std of Reward: 0.229702097062.\n",
      "Saved Model\n",
      "Step: 3660000. Mean Reward: 2.98909952607. Std of Reward: 0.150825986641.\n",
      "Step: 3670000. Mean Reward: 2.98202459792. Std of Reward: 0.238567284937.\n",
      "Step: 3680000. Mean Reward: 2.99026590693. Std of Reward: 0.129696576085.\n",
      "Step: 3690000. Mean Reward: 2.986492891. Std of Reward: 0.184390950053.\n",
      "Step: 3700000. Mean Reward: 2.99620853081. Std of Reward: 0.0721033626368.\n",
      "Saved Model\n",
      "Step: 3710000. Mean Reward: 2.98173624288. Std of Reward: 0.200712494341.\n",
      "Step: 3720000. Mean Reward: 2.99264705882. Std of Reward: 0.140328682705.\n",
      "Step: 3730000. Mean Reward: 2.99001901141. Std of Reward: 0.159148159024.\n",
      "Step: 3740000. Mean Reward: 2.99169829222. Std of Reward: 0.14848974069.\n",
      "Step: 3750000. Mean Reward: 2.98786869648. Std of Reward: 0.177619918029.\n",
      "Saved Model\n",
      "Step: 3760000. Mean Reward: 2.9933460076. Std of Reward: 0.123145644911.\n",
      "Step: 3770000. Mean Reward: 2.9860056926. Std of Reward: 0.18217816614.\n",
      "Step: 3780000. Mean Reward: 2.98268500949. Std of Reward: 0.204019154473.\n",
      "Step: 3790000. Mean Reward: 2.98697916667. Std of Reward: 0.192197380053.\n",
      "Step: 3800000. Mean Reward: 2.98555871212. Std of Reward: 0.191787520429.\n",
      "Saved Model\n",
      "Step: 3810000. Mean Reward: 2.9943019943. Std of Reward: 0.116700491498.\n",
      "Step: 3820000. Mean Reward: 2.99192782526. Std of Reward: 0.149964815483.\n",
      "Step: 3830000. Mean Reward: 2.98835551331. Std of Reward: 0.177567678518.\n",
      "Step: 3840000. Mean Reward: 2.99573459716. Std of Reward: 0.0827879475121.\n",
      "Step: 3850000. Mean Reward: 2.99310836502. Std of Reward: 0.14048576659.\n",
      "Saved Model\n",
      "Step: 3860000. Mean Reward: 2.98933649289. Std of Reward: 0.169869019921.\n",
      "Step: 3870000. Mean Reward: 2.99191246432. Std of Reward: 0.109302154319.\n",
      "Step: 3880000. Mean Reward: 2.99240265907. Std of Reward: 0.151955350544.\n",
      "Step: 3890000. Mean Reward: 2.99192015209. Std of Reward: 0.151611499749.\n",
      "Step: 3900000. Mean Reward: 2.98529411765. Std of Reward: 0.181959237109.\n",
      "Saved Model\n",
      "Step: 3910000. Mean Reward: 2.99004739336. Std of Reward: 0.164419120065.\n",
      "Step: 3920000. Mean Reward: 2.99453422053. Std of Reward: 0.0887228337261.\n",
      "Step: 3930000. Mean Reward: 2.99167459562. Std of Reward: 0.148300402024.\n",
      "Step: 3940000. Mean Reward: 2.98645437262. Std of Reward: 0.186572738366.\n",
      "Step: 3950000. Mean Reward: 2.99691650854. Std of Reward: 0.0638909074723.\n",
      "Saved Model\n",
      "Step: 3960000. Mean Reward: 2.99619771863. Std of Reward: 0.0643761439008.\n",
      "Step: 3970000. Mean Reward: 2.9943019943. Std of Reward: 0.106041669522.\n",
      "Step: 3980000. Mean Reward: 2.99335232669. Std of Reward: 0.135920296343.\n",
      "Step: 3990000. Mean Reward: 2.98674242424. Std of Reward: 0.188919861205.\n",
      "Step: 4000000. Mean Reward: 2.98719165085. Std of Reward: 0.186605773472.\n",
      "Saved Model\n",
      "Step: 4010000. Mean Reward: 2.99192782526. Std of Reward: 0.136714210738.\n",
      "Step: 4020000. Mean Reward: 2.99691064639. Std of Reward: 0.0861207680687.\n",
      "Step: 4030000. Mean Reward: 2.99049429658. Std of Reward: 0.14388664927.\n",
      "Step: 4040000. Mean Reward: 2.98528015195. Std of Reward: 0.191576759181.\n",
      "Step: 4050000. Mean Reward: 2.99312144213. Std of Reward: 0.12421529962.\n",
      "Saved Model\n",
      "Step: 4060000. Mean Reward: 2.99240986717. Std of Reward: 0.151883428464.\n",
      "Step: 4070000. Mean Reward: 2.99287072243. Std of Reward: 0.123600647933.\n",
      "Step: 4080000. Mean Reward: 2.99477186312. Std of Reward: 0.121759304292.\n",
      "Step: 4090000. Mean Reward: 2.99643536122. Std of Reward: 0.0901473898983.\n",
      "Step: 4100000. Mean Reward: 2.99763033175. Std of Reward: 0.0634257100186.\n",
      "Saved Model\n",
      "Step: 4110000. Mean Reward: 2.9869544592. Std of Reward: 0.199342191022.\n",
      "Step: 4120000. Mean Reward: 2.98907882241. Std of Reward: 0.166664975654.\n",
      "Step: 4130000. Mean Reward: 2.9853219697. Std of Reward: 0.191923799515.\n",
      "Step: 4140000. Mean Reward: 2.98977164605. Std of Reward: 0.144112313463.\n",
      "Step: 4150000. Mean Reward: 2.98410815939. Std of Reward: 0.207591482474.\n",
      "Saved Model\n",
      "Step: 4160000. Mean Reward: 2.97819905213. Std of Reward: 0.244122528087.\n",
      "Step: 4170000. Mean Reward: 2.98811787072. Std of Reward: 0.184283819231.\n",
      "Step: 4180000. Mean Reward: 2.99144486692. Std of Reward: 0.134118030853.\n",
      "Step: 4190000. Mean Reward: 2.9940645774. Std of Reward: 0.137040360504.\n",
      "Step: 4200000. Mean Reward: 2.99192782526. Std of Reward: 0.140144364974.\n",
      "Saved Model\n",
      "Step: 4210000. Mean Reward: 2.99263307985. Std of Reward: 0.097523548725.\n",
      "Step: 4220000. Mean Reward: 2.99763033175. Std of Reward: 0.0510001935633.\n",
      "Step: 4230000. Mean Reward: 2.98409306743. Std of Reward: 0.222587432546.\n",
      "Step: 4240000. Mean Reward: 2.99169040836. Std of Reward: 0.14368568324.\n",
      "Step: 4250000. Mean Reward: 2.98955365622. Std of Reward: 0.166695411249.\n",
      "Saved Model\n",
      "Step: 4260000. Mean Reward: 2.98156899811. Std of Reward: 0.222261948573.\n",
      "Step: 4270000. Mean Reward: 2.98127962085. Std of Reward: 0.234101635707.\n",
      "Step: 4280000. Mean Reward: 2.98192197907. Std of Reward: 0.193634469812.\n",
      "Step: 4290000. Mean Reward: 2.99620493359. Std of Reward: 0.0973306914856.\n",
      "Step: 4300000. Mean Reward: 2.99360189573. Std of Reward: 0.0930999198039.\n",
      "Saved Model\n",
      "Step: 4310000. Mean Reward: 2.99217267552. Std of Reward: 0.134255712613.\n",
      "Step: 4320000. Mean Reward: 2.97980038023. Std of Reward: 0.243759094751.\n",
      "Step: 4330000. Mean Reward: 2.98268500949. Std of Reward: 0.19357934162.\n",
      "Step: 4340000. Mean Reward: 2.99311490978. Std of Reward: 0.106808057536.\n",
      "Step: 4350000. Mean Reward: 2.98861480076. Std of Reward: 0.185423198218.\n",
      "Saved Model\n",
      "Step: 4360000. Mean Reward: 2.99478672986. Std of Reward: 0.123998650865.\n",
      "Step: 4370000. Mean Reward: 2.98955365622. Std of Reward: 0.168466324207.\n",
      "Step: 4380000. Mean Reward: 3.0. Std of Reward: 0.0.\n",
      "Step: 4390000. Mean Reward: 2.99193548387. Std of Reward: 0.12997756197.\n",
      "Step: 4400000. Mean Reward: 2.98124406458. Std of Reward: 0.235585529208.\n",
      "Saved Model\n",
      "Step: 4410000. Mean Reward: 2.97798295455. Std of Reward: 0.246039853597.\n",
      "Step: 4420000. Mean Reward: 2.98672985782. Std of Reward: 0.174334803484.\n",
      "Step: 4430000. Mean Reward: 2.98290598291. Std of Reward: 0.214485642587.\n",
      "Step: 4440000. Mean Reward: 2.98387096774. Std of Reward: 0.179210810667.\n",
      "Step: 4450000. Mean Reward: 2.99478672986. Std of Reward: 0.116103104262.\n",
      "Saved Model\n",
      "Step: 4460000. Mean Reward: 2.9864800759. Std of Reward: 0.208613933672.\n",
      "Step: 4470000. Mean Reward: 2.9897521449. Std of Reward: 0.164329343207.\n",
      "Step: 4480000. Mean Reward: 2.99358365019. Std of Reward: 0.112308940587.\n",
      "Step: 4490000. Mean Reward: 2.99431279621. Std of Reward: 0.083415688213.\n",
      "Step: 4500000. Mean Reward: 2.99501424501. Std of Reward: 0.120486213412.\n",
      "Saved Model\n",
      "Step: 4510000. Mean Reward: 2.99076704545. Std of Reward: 0.162025761061.\n",
      "Step: 4520000. Mean Reward: 2.99170616114. Std of Reward: 0.159203970365.\n",
      "Step: 4530000. Mean Reward: 2.98930608365. Std of Reward: 0.149673895968.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4540000. Mean Reward: 2.98601895735. Std of Reward: 0.167872891491.\n",
      "Step: 4550000. Mean Reward: 2.9926679281. Std of Reward: 0.140129945244.\n",
      "Saved Model\n",
      "Step: 4560000. Mean Reward: 2.98977164605. Std of Reward: 0.156381980058.\n",
      "Step: 4570000. Mean Reward: 2.98339658444. Std of Reward: 0.226000705755.\n",
      "Step: 4580000. Mean Reward: 2.99619771863. Std of Reward: 0.0930564185267.\n",
      "Step: 4590000. Mean Reward: 2.985. Std of Reward: 0.19988389487.\n",
      "Step: 4600000. Mean Reward: 2.9990512334. Std of Reward: 0.0217596678201.\n",
      "Saved Model\n",
      "Step: 4610000. Mean Reward: 2.97329867675. Std of Reward: 0.269439289693.\n",
      "Step: 4620000. Mean Reward: 2.99145299145. Std of Reward: 0.148746190461.\n",
      "Step: 4630000. Mean Reward: 2.98106060606. Std of Reward: 0.235610932426.\n",
      "Step: 4640000. Mean Reward: 2.99407582938. Std of Reward: 0.128886863698.\n",
      "Step: 4650000. Mean Reward: 2.99074074074. Std of Reward: 0.151667597199.\n",
      "Saved Model\n",
      "Step: 4660000. Mean Reward: 2.99573055028. Std of Reward: 0.0828271071201.\n",
      "Step: 4670000. Mean Reward: 2.99074074074. Std of Reward: 0.16298562391.\n",
      "Step: 4680000. Mean Reward: 2.99572243346. Std of Reward: 0.0746076617041.\n",
      "Step: 4690000. Mean Reward: 2.99550189394. Std of Reward: 0.10678290599.\n",
      "Step: 4700000. Mean Reward: 2.98887310606. Std of Reward: 0.170106804117.\n",
      "Saved Model\n",
      "Step: 4710000. Mean Reward: 2.9874288425. Std of Reward: 0.183578523141.\n",
      "Step: 4720000. Mean Reward: 2.98670465337. Std of Reward: 0.185384447304.\n",
      "Step: 4730000. Mean Reward: 2.99099526066. Std of Reward: 0.150164065019.\n",
      "Step: 4740000. Mean Reward: 2.98929590866. Std of Reward: 0.165586001963.\n",
      "Step: 4750000. Mean Reward: 2.99192782526. Std of Reward: 0.11857970226.\n",
      "Saved Model\n",
      "Step: 4760000. Mean Reward: 2.98624288425. Std of Reward: 0.164222975826.\n",
      "Step: 4770000. Mean Reward: 2.9912405303. Std of Reward: 0.134070434993.\n",
      "Step: 4780000. Mean Reward: 2.99408143939. Std of Reward: 0.132450351605.\n",
      "Step: 4790000. Mean Reward: 2.99571836346. Std of Reward: 0.0822248468423.\n",
      "Step: 4800000. Mean Reward: 2.99643536122. Std of Reward: 0.0742461006681.\n",
      "Saved Model\n",
      "Step: 4810000. Mean Reward: 2.99810246679. Std of Reward: 0.0435193356402.\n",
      "Step: 4820000. Mean Reward: 2.99407582938. Std of Reward: 0.136910646246.\n",
      "Step: 4830000. Mean Reward: 2.99311490978. Std of Reward: 0.150221688336.\n",
      "Step: 4840000. Mean Reward: 2.99596007605. Std of Reward: 0.0788783676629.\n",
      "Step: 4850000. Mean Reward: 2.98955365622. Std of Reward: 0.160897560264.\n",
      "Saved Model\n",
      "Step: 4860000. Mean Reward: 2.9883665717. Std of Reward: 0.161368760983.\n",
      "Step: 4870000. Mean Reward: 2.98767772512. Std of Reward: 0.177435150497.\n",
      "Step: 4880000. Mean Reward: 2.99240986717. Std of Reward: 0.143036595606.\n",
      "Step: 4890000. Mean Reward: 2.99501897533. Std of Reward: 0.115400258898.\n",
      "Step: 4900000. Mean Reward: 2.98741690408. Std of Reward: 0.151819524671.\n",
      "Saved Model\n",
      "Step: 4910000. Mean Reward: 2.99691943128. Std of Reward: 0.100011791493.\n",
      "Step: 4920000. Mean Reward: 2.9883665717. Std of Reward: 0.184372837772.\n",
      "Step: 4930000. Mean Reward: 2.99096958175. Std of Reward: 0.160690676056.\n",
      "Step: 4940000. Mean Reward: 2.99739089184. Std of Reward: 0.0620286778917.\n",
      "Step: 4950000. Mean Reward: 2.99312144213. Std of Reward: 0.11322690714.\n",
      "Saved Model\n",
      "Step: 4960000. Mean Reward: 2.99739583333. Std of Reward: 0.0600294951309.\n",
      "Step: 4970000. Mean Reward: 2.99881291548. Std of Reward: 0.038502551615.\n",
      "Step: 4980000. Mean Reward: 2.98320719016. Std of Reward: 0.232249555096.\n",
      "Step: 4990000. Mean Reward: 2.98766603416. Std of Reward: 0.17550322995.\n",
      "Step: 5000000. Mean Reward: 2.98885199241. Std of Reward: 0.170267428092.\n",
      "Saved Model\n",
      "Step: 5010000. Mean Reward: 2.98645437262. Std of Reward: 0.206522618676.\n",
      "Step: 5020000. Mean Reward: 2.98956356736. Std of Reward: 0.166972140414.\n",
      "Step: 5030000. Mean Reward: 3.0. Std of Reward: 0.0.\n",
      "Step: 5040000. Mean Reward: 2.99266098485. Std of Reward: 0.143533673303.\n",
      "Step: 5050000. Mean Reward: 2.99549335863. Std of Reward: 0.114387625613.\n",
      "Saved Model\n",
      "Step: 5060000. Mean Reward: 2.98862559242. Std of Reward: 0.184052607123.\n",
      "Step: 5070000. Mean Reward: 2.97277462121. Std of Reward: 0.284594625247.\n",
      "Step: 5080000. Mean Reward: 2.98861480076. Std of Reward: 0.160017722006.\n",
      "Step: 5090000. Mean Reward: 2.9864800759. Std of Reward: 0.186078172861.\n",
      "Step: 5100000. Mean Reward: 2.98574144487. Std of Reward: 0.18667880162.\n",
      "Saved Model\n",
      "Step: 5110000. Mean Reward: 2.99335232669. Std of Reward: 0.132380736294.\n",
      "Step: 5120000. Mean Reward: 2.99358365019. Std of Reward: 0.0988007444765.\n",
      "Step: 5130000. Mean Reward: 2.96966824645. Std of Reward: 0.29695035072.\n",
      "Step: 5140000. Mean Reward: 2.98720379147. Std of Reward: 0.185881405357.\n",
      "Step: 5150000. Mean Reward: 2.96803977273. Std of Reward: 0.306547617182.\n",
      "Saved Model\n",
      "Step: 5160000. Mean Reward: 2.9836492891. Std of Reward: 0.218851916514.\n",
      "Step: 5170000. Mean Reward: 2.99525616698. Std of Reward: 0.0820820205504.\n",
      "Step: 5180000. Mean Reward: 2.99407020873. Std of Reward: 0.0918936292324.\n",
      "Step: 5190000. Mean Reward: 2.99523355577. Std of Reward: 0.0981291525511.\n",
      "Step: 5200000. Mean Reward: 2.9864800759. Std of Reward: 0.18543973351.\n",
      "Saved Model\n",
      "Step: 5210000. Mean Reward: 2.99691358025. Std of Reward: 0.0709618434434.\n",
      "Step: 5220000. Mean Reward: 2.99312144213. Std of Reward: 0.111112326943.\n",
      "Step: 5230000. Mean Reward: 2.98816287879. Std of Reward: 0.185856524537.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fea09393b070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Decide and take an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deepthy/GitHub/UnityRLearning/RLearningUnity3D/python/ppo/trainer.pyc\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_variance\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mrun_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_variance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m           \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m           \u001b[0mfeed_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operation was not named: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s:%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-5200000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-5200000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
